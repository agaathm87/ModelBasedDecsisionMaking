{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c6ca43-240f-45cd-abd4-51b056c17129",
   "metadata": {},
   "source": [
    "## Week 2 Practical Assignment: Exploring Real-World Networks\n",
    "# Analysis of the SNAP YouTube Social Network\n",
    "## Course: Model Based Decisions (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc9b62-e834-41c0-bbf7-d4a7b0d65fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import warnings\n",
    "import math\n",
    "import collections\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib to display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style for better looking plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) # Random seed for reproducible model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffda09b-51c6-4bba-b3db-859327e89fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #################################################################################################### #\n",
    "# Network Visualization Suite Class (Adapted for YouTube Analysis from network_visualisation_suite.py) #\n",
    "# #################################################################################################### #\n",
    "class NetworkVisualizationSuite:\n",
    "    \"\"\"\n",
    "    Comprehensive network analysis suite, adapted for N > 1M nodes.\n",
    "    Uses sampling for path-dependent metrics (Betweenness, Closeness, Avg Path Length)\n",
    "    to ensure execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the suite.\"\"\"\n",
    "        self.networks = {}               # Dictionary to store the networks\n",
    "        self.network_stats = {}           # Dictionary to store the network statistics\n",
    "        self.youtube_url = \"https://snap.stanford.edu/data/com-youtube.ungraph.txt.gz\"\n",
    "        self.real_network_key = 'YouTube'\n",
    "        self.sampling_k = 1000            # Sampling size for all path-dependent metrics (L, Betweenness, Closeness)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    # 1. Load Real YouTube Network\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    def load_youtube_network(self):\n",
    "        \"\"\"Load YouTube social network from compressed SNAP dataset URL and return LCC.\"\"\"\n",
    "        print(f\"--- 1. Data Loading ---\")\n",
    "        \n",
    "        # Load the dataset using pandas\n",
    "        youtube_df = pd.read_csv(\n",
    "            self.youtube_url,\n",
    "            compression=\"gzip\",\n",
    "            sep=\"\\t\",\n",
    "            comment=\"#\",\n",
    "            names=[\"start_node\", \"end_node\"],\n",
    "        )\n",
    "        \n",
    "        # Create the graph object G from the pandas edgelist \n",
    "        G = nx.from_pandas_edgelist(youtube_df, \"start_node\", \"end_node\")\n",
    "        \n",
    "        # Check connectivity and get LCC (which for YouTube is the entire graph)\n",
    "        if nx.is_connected(G):\n",
    "            G_lcc = G.copy()\n",
    "            print(\"Graph is fully connected. Proceeding with the full graph.\")\n",
    "        else:\n",
    "            # Fallback for LCC calculation\n",
    "            largest_cc_nodes = max(nx.connected_components(G), key=len)\n",
    "            G_lcc = G.subgraph(largest_cc_nodes).copy()\n",
    "\n",
    "        print(f\"Loaded YouTube network: {G_lcc.number_of_nodes():,} nodes, {G_lcc.number_of_edges():,} edges\\n\")\n",
    "        self.networks[self.real_network_key] = G_lcc \n",
    "        return G_lcc\n",
    "    \n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    # 2. Generate Theoretical Network Models (ER, WS, BA)\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    def generate_theoretical_networks(self, G_lcc):\n",
    "        \"\"\"Generate theoretical network models (ER, WS, BA) for comparison.\"\"\"\n",
    "        \n",
    "        n = G_lcc.number_of_nodes()\n",
    "        e = G_lcc.number_of_edges()\n",
    "        avg_deg = (2 * e) / n\n",
    "        \n",
    "        print(f\"--- 2. Model Generation ---\")\n",
    "        \n",
    "        # 1. Erdős-Rényi (ER) Model \n",
    "        p_er = avg_deg / (n - 1)\n",
    "        er_graph = nx.erdos_renyi_graph(n, p_er, seed=seed) \n",
    "        \n",
    "        # 2. Watts-Strogatz (small-world) \n",
    "        k = max(4, int(round(avg_deg))) \n",
    "        k = k if k % 2 == 0 else k + 1     # Ensure k is even\n",
    "        k_ws = k \n",
    "        p_ws = 0.1 \n",
    "        ws_graph = nx.watts_strogatz_graph(n, k_ws, p_ws, seed=seed)\n",
    "        \n",
    "        # 3. Barabási-Albert (scale-free) \n",
    "        m = max(2, int(round(avg_deg / 2)))   # m ≈ <k> / 2\n",
    "        m_ba = m\n",
    "        ba_graph = nx.barabasi_albert_graph(n, m_ba, seed=seed)\n",
    "        \n",
    "        # Store all theoretical models\n",
    "        self.networks.update({\n",
    "            \"Erdős-Rényi\": er_graph,\n",
    "            \"Watts-Strogatz\": ws_graph,\n",
    "            \"Barabási-Albert\": ba_graph\n",
    "        })\n",
    "        \n",
    "        print(f\"Theoretical networks generated successfully (ER p={p_er:.8f}, WS k={k_ws}, BA m={m_ba})!\\n\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    # 3. Approximate Average Path Length (Sampling)\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    def _approximate_avg_path_length(self, G, k):\n",
    "        \"\"\"Approximates Average Shortest Path Length by sampling k source nodes.\"\"\"\n",
    "        nodes = list(G.nodes())\n",
    "        sampled_nodes = np.random.choice(nodes, min(len(nodes), k), replace=False)\n",
    "        total_path_length = 0\n",
    "        num_paths = 0\n",
    "        \n",
    "        for source in sampled_nodes:\n",
    "            try:\n",
    "                length = nx.shortest_path_length(G, source=source)\n",
    "            except nx.NetworkXNoPath:\n",
    "                # Skip if the sampled component is somehow disconnected\n",
    "                continue \n",
    "            \n",
    "            # Sum path lengths and count valid paths\n",
    "            for target, path_len in length.items():\n",
    "                if source != target:\n",
    "                    total_path_length += path_len\n",
    "                    num_paths += 1\n",
    "                    \n",
    "        if num_paths > 0:\n",
    "            return total_path_length / num_paths\n",
    "        return float('nan')    # Return NaN if path computation fails\n",
    "\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    # 4. Analyze Network Properties (Structural Metrics)\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    def analyze_network_properties(self):\n",
    "        \"\"\"\n",
    "        Computes network metrics. Uses sampling for Avg. Path Length due to N > 1M.\n",
    "        \"\"\"\n",
    "        print(\"--- 3. Analyzing Network Properties ---\")\n",
    "        \n",
    "        for name, G in self.networks.items():\n",
    "            print(f\"Analyzing {name} network...\")\n",
    "            \n",
    "            stats = {}\n",
    "            stats['nodes'] = G.number_of_nodes()\n",
    "            stats['edges'] = G.number_of_edges()\n",
    "            stats['avg_degree'] = np.mean([d for _, d in G.degree()])\n",
    "            \n",
    "            # Clustering, Assortativity (All feasible exactly)\n",
    "            stats['avg_clustering'] = nx.average_clustering(G)\n",
    "            stats['assortativity'] = nx.degree_assortativity_coefficient(G) \n",
    "            \n",
    "            # Path lengths (Approximated via Sampling)\n",
    "            k_sample = 250 if name != self.real_network_key else self.sampling_k \n",
    "            \n",
    "            try:\n",
    "                 stats['avg_path_length'] = self._approximate_avg_path_length(G, k_sample)\n",
    "            except Exception as e:\n",
    "                 print(f\"  Warning: Path length calculation failed for {name}: {e}\")\n",
    "                 stats['avg_path_length'] = np.nan\n",
    "                \n",
    "            self.network_stats[name] = stats\n",
    "        \n",
    "        print(\"\\nAnalysis of network properties complete.\\n\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    # 5. Centrality Analysis (with sampling)\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    def run_centrality_analysis(self, top_n=5):\n",
    "        \"\"\"Calculates all four core centrality measures, using sampling for intractable ones.\"\"\"\n",
    "        G_lcc = self.networks[self.real_network_key]\n",
    "        print(f\"\\n--- 4. Centrality Analysis (Top {top_n} Nodes) ---\")\n",
    "        \n",
    "        def print_top_nodes(centrality_dict, name):\n",
    "            \"\"\"Helper function to sort and print top N nodes.\"\"\"\n",
    "            sorted_nodes = sorted(centrality_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "            print(f\"\\n--- Top {top_n} Nodes by {name} ---\")\n",
    "            for i in range(top_n):\n",
    "                node, score = sorted_nodes[i]\n",
    "                print(f\"  {i+1}. Node {node}: {score:.4f}\")\n",
    "            return sorted_nodes\n",
    "\n",
    "        # Store summary centralities for later table output\n",
    "        centrality_summary = {}\n",
    "\n",
    "        # 1. Degree Centrality (Feasible) \n",
    "        deg_cent = nx.degree_centrality(G_lcc)\n",
    "        sorted_deg = print_top_nodes(deg_cent, \"Degree Centrality\")\n",
    "        centrality_summary[\"deg_avg\"] = np.mean(list(deg_cent.values()))\n",
    "        centrality_summary[\"deg_max\"] = sorted_deg[0][1]\n",
    "\n",
    "        # 2. Eigenvector Centrality (Feasible) \n",
    "        try:\n",
    "            eig_cent = nx.eigenvector_centrality(G_lcc, max_iter=1000, seed=seed) \n",
    "            sorted_eig = print_top_nodes(eig_cent, \"Eigenvector Centrality\")\n",
    "            centrality_summary[\"eig_avg\"] = np.mean(list(eig_cent.values()))\n",
    "            centrality_summary[\"eig_max\"] = sorted_eig[0][1]\n",
    "        except nx.NetworkXException as e:\n",
    "            print(f\"\\nCould not compute Eigenvector Centrality: {e}\")\n",
    "            centrality_summary[\"eig_avg\"] = np.nan\n",
    "            centrality_summary[\"eig_max\"] = np.nan\n",
    "\n",
    "        # 3. Betweenness Centrality (Approximate, using k sampled sources) \n",
    "        print(f\"\\n--- Betweenness Centrality (Computed via Sampling, k={self.sampling_k}) ---\")\n",
    "        try:\n",
    "            bet_cent = nx.betweenness_centrality(G_lcc, k=self.sampling_k, seed=seed)\n",
    "            sorted_bet = print_top_nodes(bet_cent, \"Betweenness Centrality\")\n",
    "            centrality_summary[\"bet_avg\"] = np.mean(list(bet_cent.values()))\n",
    "            centrality_summary[\"bet_max\"] = sorted_bet[0][1]\n",
    "        except Exception as e:\n",
    "            print(f\"  Calculation failed: {e}\")\n",
    "            centrality_summary[\"bet_avg\"] = np.nan\n",
    "            centrality_summary[\"bet_max\"] = np.nan\n",
    "\n",
    "        # 4. Closeness Centrality (Approximate, computing only for k sampled nodes) \n",
    "        print(f\"\\n--- Closeness Centrality (Computed via Sampling, k={self.sampling_k}) ---\")\n",
    "        try:\n",
    "            nodes_to_sample = np.random.choice(G_lcc.nodes(), self.sampling_k, replace=False)\n",
    "            clo_cent = {node: nx.closeness_centrality(G_lcc, u=node) for node in nodes_to_sample}\n",
    "            sorted_clo = print_top_nodes(clo_cent, \"Closeness Centrality\")\n",
    "            centrality_summary[\"clo_avg\"] = np.mean(list(clo_cent.values()))\n",
    "            centrality_summary[\"clo_max\"] = sorted_clo[0][1]\n",
    "        except Exception as e:\n",
    "            print(f\"  Calculation failed: {e}\")\n",
    "            centrality_summary[\"clo_avg\"] = np.nan\n",
    "            centrality_summary[\"clo_max\"] = np.nan\n",
    "\n",
    "        # Add to network statistics\n",
    "        self.network_stats[self.real_network_key].update(centrality_summary)\n",
    "        print(\"\\n--- Centrality Analysis Complete ---\\n\")\n",
    "        \n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    # 6. Print Summary Table (Includes Centrality Measures)\n",
    "    # -------------------------------------------------------------------------------- #\n",
    "    def print_summary_statistics(self):\n",
    "        \"\"\"Prints the comparative summary table including centrality metrics.\"\"\"\n",
    "        stats_data = []\n",
    "        for name, stats in self.network_stats.items():\n",
    "            stats_data.append({\n",
    "                \"Network\": name,\n",
    "                \"Nodes\": f\"{stats['nodes']:,}\",\n",
    "                \"Edges\": f\"{stats['edges']:,}\",\n",
    "                \"Avg. Clustering\": f\"{stats['avg_clustering']:.4f}\",\n",
    "                \"Avg. Path Length\": f\"{stats['avg_path_length']:.2f}\",\n",
    "                \"Assortativity\": f\"{stats['assortativity']:.4f}\",\n",
    "                \"Avg Degree Cent.\": f\"{stats.get('deg_avg', np.nan):.4f}\" if not np.isnan(stats.get('deg_avg', np.nan)) else \"N/A\",\n",
    "                \"Max Degree Cent.\": f\"{stats.get('deg_max', np.nan):.4f}\" if not np.isnan(stats.get('deg_max', np.nan)) else \"N/A\",\n",
    "                \"Avg Eigenvector Cent.\": f\"{stats.get('eig_avg', np.nan):.4f}\" if not np.isnan(stats.get('eig_avg', np.nan)) else \"N/A\",\n",
    "                \"Max Eigenvector Cent.\": f\"{stats.get('eig_max', np.nan):.4f}\" if not np.isnan(stats.get('eig_max', np.nan)) else \"N/A\",\n",
    "                \"Avg Betweenness Cent.\": f\"{stats.get('bet_avg', np.nan):.4f}\" if not np.isnan(stats.get('bet_avg', np.nan)) else \"N/A\",\n",
    "                \"Max Betweenness Cent.\": f\"{stats.get('bet_max', np.nan):.4f}\" if not np.isnan(stats.get('bet_max', np.nan)) else \"N/A\",\n",
    "                \"Avg Closeness Cent.\": f\"{stats.get('clo_avg', np.nan):.4f}\" if not np.isnan(stats.get('clo_avg', np.nan)) else \"N/A\",\n",
    "                \"Max Closeness Cent.\": f\"{stats.get('clo_max', np.nan):.4f}\" if not np.isnan(stats.get('clo_max', np.nan)) else \"N/A\"\n",
    "            })\n",
    "            \n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        print(\"\\n--- Summary Statistics Table ---\")\n",
    "        print(stats_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------- #\n",
    "# MAIN FUNCTION\n",
    "# -------------------------------------------------------------------------------- #\n",
    "def main_analysis():\n",
    "    \"\"\"Main function to run the network visualization suite.\"\"\"\n",
    "    print(\"Network Visualization Suite for Week Two - Networks\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize suite\n",
    "    suite = NetworkVisualizationSuite()\n",
    "    \n",
    "    # 1. Load Real Network Data (YouTube)\n",
    "    G_lcc = suite.load_youtube_network()\n",
    "    \n",
    "    # 2. Generate Theoretical Networks\n",
    "    suite.generate_theoretical_networks(G_lcc)\n",
    "    \n",
    "    # 3. Analyze Properties of all networks (Real and Models)\n",
    "    suite.analyze_network_properties()\n",
    "    \n",
    "    # 4. Run Centrality Analysis (including sampled metrics)\n",
    "    suite.run_centrality_analysis()\n",
    "    \n",
    "    # 5. Print Summary Table\n",
    "    suite.print_summary_statistics()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------- #\n",
    "# RUN SCRIPT\n",
    "# -------------------------------------------------------------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    main_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427c4c0-ce67-4fbe-9b7a-158555c38ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2cfe5a-51f9-4c36-a600-fdcfd00ea642",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3780400585.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    for /r %%v in (*.py) do p2j \"%%v\"\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075aa3a-718b-4cfb-9b6e-50ef388b6807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
