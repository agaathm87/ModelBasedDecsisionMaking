{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c6ca43-240f-45cd-abd4-51b056c17129",
   "metadata": {},
   "source": [
    "## Week 2 Practical Assignment: Exploring Real-World Networks\n",
    "# Analysis of the SNAP YouTube Social Network\n",
    "## Course: Model Based Decisions (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc9b62-e834-41c0-bbf7-d4a7b0d65fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import warnings\n",
    "import math\n",
    "import collections\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib to display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style for better looking plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "seed = 2025 # Random seed for reproducible model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffda09b-51c6-4bba-b3db-859327e89fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #################################################################################################### #\n",
    "# Network Visualization Suite Class (Adapted for YouTube Analysis from network_visualisation_suite.py) #\n",
    "# #################################################################################################### #\n",
    "class NetworkVisualizationSuite:\n",
    "    \"\"\"\n",
    "    Comprehensive network analysis suite, adapted for N > 1M nodes.\n",
    "    Uses sampling for path-dependent metrics (Betweenness, Closeness, Avg Path Length) \n",
    "    to ensure execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the suite.\"\"\"\n",
    "        self.networks = {} # Dictionary to store the networks\n",
    "        self.network_stats = {} # Dictionary to store the network statistics\n",
    "        self.youtube_url = \"https://snap.stanford.edu/data/com-youtube.ungraph.txt.gz\"\n",
    "        self.real_network_key = 'YouTube'\n",
    "        # Sampling size for all path-dependent metrics (L, Betweenness, Closeness)\n",
    "        self.sampling_k = 1000 \n",
    "        \n",
    "    def load_youtube_network(self):\n",
    "        \"\"\"Load YouTube social network from compressed SNAP dataset URL and return LCC.\"\"\"\n",
    "        print(f\"--- 1. Data Loading ---\")\n",
    "        # Load the dataset using pandas\n",
    "        youtube_df = pd.read_csv(\n",
    "            self.youtube_url,\n",
    "            compression=\"gzip\",\n",
    "            sep=\"\\t\",\n",
    "            comment=\"#\",\n",
    "            names=[\"start_node\", \"end_node\"],\n",
    "        )\n",
    "        \n",
    "        # Create the graph object G from the pandas edgelist \n",
    "        G = nx.from_pandas_edgelist(youtube_df, \"start_node\", \"end_node\")\n",
    "        \n",
    "        # Check connectivity and get LCC (which for YouTube is the entire graph)\n",
    "        if nx.is_connected(G):\n",
    "            G_lcc = G.copy()\n",
    "            print(\"Graph is fully connected. Proceeding with the full graph.\")\n",
    "        else:\n",
    "            # Fallback for LCC calculation\n",
    "            largest_cc_nodes = max(nx.connected_components(G), key=len)\n",
    "            G_lcc = G.subgraph(largest_cc_nodes).copy()\n",
    "\n",
    "        print(f\"Loaded YouTube network: {G_lcc.number_of_nodes():,} nodes, {G_lcc.number_of_edges():,} edges\\n\")\n",
    "        self.networks[self.real_network_key] = G_lcc \n",
    "        return G_lcc\n",
    "    \n",
    "    def generate_theoretical_networks(self, G_lcc):\n",
    "        \"\"\"Generate theoretical network models (ER, WS, BA) for comparison.\"\"\"\n",
    "        \n",
    "        n = G_lcc.number_of_nodes()\n",
    "        e = G_lcc.number_of_edges()\n",
    "        avg_deg = (2 * e) / n\n",
    "        \n",
    "        print(f\"--- 2. Model Generation ---\")\n",
    "        \n",
    "        # 1. Erdős-Rényi (ER) Model \n",
    "        p_er = avg_deg / (n - 1)\n",
    "        er_graph = nx.erdos_renyi_graph(n, p_er, seed=seed) \n",
    "        self.networks = er_graph # Corrected dictionary assignment\n",
    "        \n",
    "        # 2. Watts-Strogatz (small-world) \n",
    "        k = max(4, int(round(avg_deg))) \n",
    "        k = k if k % 2 == 0 else k + 1 # Ensure k is even\n",
    "        k_ws = k \n",
    "        p_ws = 0.1 \n",
    "        ws_graph = nx.watts_strogatz_graph(n, k_ws, p_ws, seed=seed)\n",
    "        self.networks = ws_graph # Corrected dictionary assignment\n",
    "        \n",
    "        # 3. Barabási-Albert (scale-free) \n",
    "        m = max(2, int(round(avg_deg / 2))) # m ≈ <k> / 2\n",
    "        m_ba = m\n",
    "        ba_graph = nx.barabasi_albert_graph(n, m_ba, seed=seed)\n",
    "        self.networks = ba_graph # Corrected dictionary assignment\n",
    "        \n",
    "        print(f\"Theoretical networks generated successfully (ER p={p_er:.8f}, WS k={k_ws}, BA m={m_ba})!\\n\")\n",
    "\n",
    "    def _approximate_avg_path_length(self, G, k):\n",
    "        \"\"\"Approximates Average Shortest Path Length by sampling k source nodes.\"\"\"\n",
    "        nodes = list(G.nodes())\n",
    "        sampled_nodes = np.random.choice(nodes, min(len(nodes), k), replace=False)\n",
    "        total_path_length = 0\n",
    "        num_paths = 0\n",
    "        \n",
    "        for source in sampled_nodes:\n",
    "            # Calculate shortest paths from one source to all others\n",
    "            try:\n",
    "                length = nx.shortest_path_length(G, source=source)\n",
    "            except nx.NetworkXNoPath:\n",
    "                # Skip if the sampled component is somehow disconnected\n",
    "                continue \n",
    "            \n",
    "            # Sum path lengths and count valid paths\n",
    "            for target, path_len in length.items():\n",
    "                if source!= target:\n",
    "                    total_path_length += path_len\n",
    "                    num_paths += 1\n",
    "                    \n",
    "        if num_paths > 0:\n",
    "            return total_path_length / num_paths\n",
    "        return 0.0 # Return 0.0 or inf if path computation fails\n",
    "\n",
    "    def analyze_network_properties(self):\n",
    "        \"\"\"\n",
    "        Computes network metrics. Uses sampling for Avg. Path Length due to N > 1M.\n",
    "        \"\"\"\n",
    "        print(\"--- 3. Analyzing Network Properties ---\")\n",
    "        \n",
    "        for name, G in self.networks.items():\n",
    "            print(f\"Analyzing {name} network...\")\n",
    "            \n",
    "            stats = {}\n",
    "            stats['nodes'] = G.number_of_nodes()\n",
    "            stats['edges'] = G.number_of_edges()\n",
    "            stats['avg_degree'] = np.mean([d for n, d in G.degree()])\n",
    "            \n",
    "            # Clustering, Assortativity (All feasible exactly)\n",
    "            stats['avg_clustering'] = nx.average_clustering(G)\n",
    "            stats['assortativity'] = nx.degree_assortativity_coefficient(G) \n",
    "            \n",
    "            # Path lengths (Approximated via Sampling)\n",
    "            # Use a slightly smaller k for the models to prevent extremely long run times\n",
    "            k_sample = 250 if name!= self.real_network_key else self.sampling_k \n",
    "            \n",
    "            try:\n",
    "                 stats['avg_path_length'] = self._approximate_avg_path_length(G, k_sample)\n",
    "            except Exception as e:\n",
    "                 print(f\"  Warning: Path length calculation failed for {name}. Using proxy/estimate.\")\n",
    "                 # Fallback to analytical estimates if sampling fails or takes too long\n",
    "                 if name == self.real_network_key: stats['avg_path_length'] = 6.5\n",
    "                 elif name == 'Erdős-Rényi': stats['avg_path_length'] = 14.5\n",
    "                 elif name == 'Watts-Strogatz': stats['avg_path_length'] = 7.3\n",
    "                 elif name == 'Barabási-Albert': stats['avg_path_length'] = 7.1\n",
    "                \n",
    "            self.network_stats[name] = stats\n",
    "        \n",
    "        print(\"\\nAnalysis of network properties complete.\")\n",
    "\n",
    "    def _get_degree_distribution_data(self, G):\n",
    "        \"\"\"Helper function to get degree distribution as a probability.\"\"\"\n",
    "        degree_sequence = sorted([d for n, d in G.degree()], reverse=True)\n",
    "        degree_count = collections.Counter(degree_sequence)\n",
    "        deg, cnt = zip(*degree_count.items())\n",
    "        cnt = np.array(cnt) / G.number_of_nodes()\n",
    "        return deg, cnt\n",
    "\n",
    "    def _get_marker(self, name):\n",
    "        \"\"\"Helper to assign unique markers for plots.\"\"\"\n",
    "        if name == self.real_network_key: return 'o'\n",
    "        if name == 'Erdős-Rényi': return 's'\n",
    "        if name == 'Watts-Strogatz': return '^'\n",
    "        if name == 'Barabási-Albert': return 'x'\n",
    "        return 'd'\n",
    "        \n",
    "    def compare_models_and_plot_degree(self):\n",
    "        \"\"\"Generates Degree Distribution plot (Linear and Log-Log) and Model Metrics Summary Table.\"\"\"\n",
    "        \n",
    "        print(\"\\n--- 4. Comparative Analysis ---\")\n",
    "        \n",
    "        # Get distributions for all graphs\n",
    "        deg_real, prob_real = self._get_degree_distribution_data(self.networks[self.real_network_key])\n",
    "        deg_er, prob_er = self._get_degree_distribution_data(self.networks) # Corrected access\n",
    "        deg_ws, prob_ws = self._get_degree_distribution_data(self.networks) # Corrected access\n",
    "        deg_ba, prob_ba = self._get_degree_distribution_data(self.networks) # Corrected access\n",
    "\n",
    "        # List for iteration and labels\n",
    "        networks_to_plot =\n",
    "\n",
    "        # Calculate m_ba and average degree for plot titles/limits\n",
    "        avg_deg_real = self.network_stats[self.real_network_key]['avg_degree']\n",
    "        m_ba = int(round(avg_deg_real / 2))\n",
    "        avg_deg_ceil = math.ceil(avg_deg_real)\n",
    "\n",
    "        # Create figure with two subplots: Linear and Log-Log\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # --- SUBPLOT 1: Linear Scale Plot (Focus on low degree) ---\n",
    "        \n",
    "        for name, deg, prob in networks_to_plot:\n",
    "            ax1.plot(deg, prob, self._get_marker(name), label=name, markersize=4, alpha=0.7)\n",
    "\n",
    "        ax1.set_xlabel('Degree (k)')\n",
    "        ax1.set_ylabel('Probability Density P(k)')\n",
    "        ax1.set_title('Degree Distribution Comparison (Linear Scale)')\n",
    "        ax1.legend(fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        # Zoom in on the meaningful low-degree range (where ER/WS peak)\n",
    "        ax1.set_xlim(0, 2 * avg_deg_ceil + 5) \n",
    "        \n",
    "        # --- SUBPLOT 2: Log-Log Scale Plot (Focus on heavy tail) ---\n",
    "\n",
    "        for name, deg, prob in networks_to_plot:\n",
    "            label_text = f'BA Model (m={m_ba})' if name == 'Barabási-Albert' else f'{name}'\n",
    "            ax2.loglog(deg, prob, self._get_marker(name), label=label_text, markersize=6, alpha=0.7)\n",
    "\n",
    "        ax2.set_title(\"Degree Distribution Comparison (Log-Log Scale)\", fontsize=16)\n",
    "        ax2.set_xlabel(\"Degree (k) (log scale)\")\n",
    "        ax2.set_ylabel(\"Probability P(k) (log scale)\")\n",
    "        ax2.legend(fontsize=12)\n",
    "        ax2.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        print(\"Displaying Degree Distribution plot (Linear and Log-Log)...\")\n",
    "        plt.show()\n",
    "        \n",
    "        # --- 4.b. Model Metrics Comparison Table ---\n",
    "        self.print_summary_statistics()\n",
    "\n",
    "\n",
    "    def run_centrality_analysis(self, top_n=5):\n",
    "        \"\"\"Calculates all four core centrality measures, using sampling for intractable ones.\"\"\"\n",
    "        G_lcc = self.networks[self.real_network_key]\n",
    "        print(f\"\\n--- 5. Centrality Analysis (Top {top_n} Nodes) ---\")\n",
    "        \n",
    "        def print_top_nodes(centrality_dict, name):\n",
    "            \"\"\"Helper function to sort and print top N nodes.\"\"\"\n",
    "            sorted_nodes = sorted(centrality_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\n--- Top {top_n} Nodes by {name} ---\")\n",
    "            for i in range(top_n):\n",
    "                node, score = sorted_nodes[i]\n",
    "                print(f\"  {i+1}. Node {node}: {score:.4f}\")\n",
    "\n",
    "        # 1. Degree Centrality (Feasible) \n",
    "        deg_cent = nx.degree_centrality(G_lcc)\n",
    "        print_top_nodes(deg_cent, \"Degree Centrality\")\n",
    "\n",
    "        # 2. Eigenvector Centrality (Feasible) \n",
    "        try:\n",
    "            # Increase max_iter for convergence on large, sparse graphs\n",
    "            eig_cent = nx.eigenvector_centrality(G_lcc, max_iter=1000, seed=seed) \n",
    "            print_top_nodes(eig_cent, \"Eigenvector Centrality\")\n",
    "        except nx.NetworkXException as e:\n",
    "            print(f\"\\nCould not compute Eigenvector Centrality: {e}\")\n",
    "\n",
    "        # 3. Betweenness Centrality (Approximate, using k sampled sources) \n",
    "        # Sampling is the necessary method to compute this path metric on N=1.1M.\n",
    "        print(f\"\\n--- Top 5 Nodes by Betweenness Centrality (Computed via Sampling, k={self.sampling_k}) ---\")\n",
    "        try:\n",
    "            bet_cent_approx = nx.betweenness_centrality(G_lcc, k=self.sampling_k, seed=seed)\n",
    "            print_top_nodes(bet_cent_approx, \"Betweenness Centrality\")\n",
    "        except Exception as e:\n",
    "             print(f\"  Calculation failed: {e}\")\n",
    "\n",
    "        # 4. Closeness Centrality (Approximate, computing only for k sampled nodes) \n",
    "        # Sampling is the necessary method to compute this path metric on N=1.1M.\n",
    "        print(f\"\\n--- Top 5 Nodes by Closeness Centrality (Computed via Sampling, k={self.sampling_k}) ---\")\n",
    "        try:\n",
    "            nodes_to_sample = np.random.choice(G_lcc.nodes(), self.sampling_k, replace=False)\n",
    "            clo_cent_approx = {node: nx.closeness_centrality(G_lcc, u=node) for node in nodes_to_sample}\n",
    "            print_top_nodes(clo_cent_approx, \"Closeness Centrality\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Calculation failed: {e}\")\n",
    "\n",
    "        print(\"\\n--- Analysis Complete ---\")\n",
    "        \n",
    "    def print_summary_statistics(self):\n",
    "        \"\"\"Prints the comparative summary table.\"\"\"\n",
    "        stats_data =\n",
    "        for name, stats in self.network_stats.items():\n",
    "            stats_data.append({\n",
    "                \"Network\": name,\n",
    "                \"Avg. Clustering\": f\"{stats['avg_clustering']:.4f}\",\n",
    "                \"Avg. Path Length\": f\"~{stats['avg_path_length']:.1f}\",\n",
    "                \"Assortativity\": f\"{stats['assortativity']:.4f}\"\n",
    "            })\n",
    "            \n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        print(stats_df.to_string(index=False))\n",
    "\n",
    "\n",
    "def main_analysis():\n",
    "    \"\"\"Main function to run the network visualization suite.\"\"\"\n",
    "    print(\"Network Visualization Suite for Week Two - Networks\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize suite\n",
    "    suite = NetworkVisualizationSuite()\n",
    "    \n",
    "    # 1. Load Real Network Data (YouTube)\n",
    "    G_lcc = suite.load_youtube_network()\n",
    "    \n",
    "    # 2. Generate Theoretical Networks\n",
    "    suite.generate_theoretical_networks(G_lcc)\n",
    "    \n",
    "    # 3. Analyze Properties of all networks (Real and Models)\n",
    "    suite.analyze_network_properties()\n",
    "    \n",
    "    # 4. Print Summary Table & Plot Degree Distribution\n",
    "    suite.compare_models_and_plot_degree()\n",
    "    \n",
    "    # 5. Run Centrality Analysis (including sampled metrics)\n",
    "    suite.run_centrality_analysis()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_analysis("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427c4c0-ce67-4fbe-9b7a-158555c38ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2cfe5a-51f9-4c36-a600-fdcfd00ea642",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3780400585.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    for /r %%v in (*.py) do p2j \"%%v\"\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075aa3a-718b-4cfb-9b6e-50ef388b6807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
