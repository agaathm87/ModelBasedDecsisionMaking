{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c6ca43-240f-45cd-abd4-51b056c17129",
   "metadata": {},
   "source": [
    "## Week 2 Practical Assignment: Exploring Real-World Networks\n",
    "# Analysis of the SNAP YouTube Social Network\n",
    "## Course: Model Based Decisions (2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffc9b62-e834-41c0-bbf7-d4a7b0d65fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import warnings\n",
    "import math\n",
    "import collections\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "**Disclaimer — sampling & approximations**\n",
    "\n",
    "This notebook uses sampling and reduced-size synthetic models to keep runtime and memory within practical limits. Path-dependent metrics (average path length, betweenness, closeness) are estimated from subsamples and therefore are approximate; results should be interpreted qualitatively rather than as exact values. For exact computations on very large graphs, consider scalable libraries (e.g., graph-tool, igraph) or running on a machine with sufficient RAM/CPU. Reproducibility is controlled by the global random seed, but numerical variability may remain due to sampling.\n",
    "\"\"\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib to display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style for better looking plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) # Random seed for reproducible model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffda09b-51c6-4bba-b3db-859327e89fd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:68\u001b[1;36m\u001b[0m\n\u001b[1;33m    def generate_theoretical_networks(self, G_lcc):\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ####################################################################################################################################################################### #\n",
    "# Network Visualization Suite Class (Adapted for YouTube Analysis from network_visualisation_suite.py; facebook_vs_erdos_renyi.py, airline_network_analysis.py bu M.Lees) #\n",
    "# ####################################################################################################################################################################### #\n",
    "\n",
    "class NetworkVisualizationSuite:\n",
    "    \"\"\"\n",
    "    Comprehensive network analysis suite, adapted for N > 1M nodes.\n",
    "    Uses sampling for path-dependent metrics (Betweenness, Closeness, Avg Path Length) \n",
    "    to ensure execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"datasets\"):\n",
    "        \"\"\"Initialize the suite.\"\"\"\n",
    "        self.networks = {} # Dictionary to store the networks\n",
    "        self.network_stats = {} # Dictionary to store the network statistics\n",
    "        self.data_dir = data_dir\n",
    "        # ensure data directory exists (no harm if it already exists)\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        self.real_network_key = 'YouTube'\n",
    "        # Sampling size for all path-dependent metrics (L, Betweenness, Closeness)\n",
    "        # Set to 10_000 as requested (adjust if you have limited RAM/CPU)\n",
    "        self.sampling_k = 10_000\n",
    "        # Cap for synthetic model generation to avoid OOM when n (YouTube) is large\n",
    "        self.model_max_n = 100_000\n",
    "        \n",
    "    def load_youtube_network(self, filename=\"com-youtube.ungraph.txt.gz\"):\n",
    "        \"\"\"Load YouTube social network from compressed SNAP dataset and return LCC.\"\"\"\n",
    "        print(f\"--- 1. Data Loading ---\")\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: {filepath} not found! Please download the SNAP file and place it in '{self.data_dir}'.\")\n",
    "            return None\n",
    "            \n",
    "        # Load the dataset using pandas (robust options for large files)\n",
    "        try:\n",
    "            youtube_df = pd.read_csv(\n",
    "                filepath,\n",
    "                compression=\"gzip\",\n",
    "                sep=\"\\t\",\n",
    "                comment=\"#\",\n",
    "                names=[\"start_node\", \"end_node\"],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filepath}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if youtube_df.shape[0] == 0:\n",
    "            print(f\"Warning: no edges loaded from {filepath}.\")\n",
    "            return None\n",
    "\n",
    "        # Create the graph object G from the pandas edgelist \n",
    "        G = nx.from_pandas_edgelist(youtube_df, \"start_node\", \"end_node\")\n",
    "        \n",
    "        # Check connectivity and get LCC (which for YouTube is often the entire graph)\n",
    "        if nx.is_connected(G):\n",
    "            G_lcc = G.copy()\n",
    "            print(\"Graph is fully connected. Proceeding with the full graph.\")\n",
    "        else:\n",
    "            # Fallback for LCC calculation\n",
    "            largest_cc_nodes = max(nx.connected_components(G), key=len)\n",
    "            G_lcc = G.subgraph(largest_cc_nodes).copy()\n",
    "\n",
    "        print(f\"Loaded YouTube network: {G_lcc.number_of_nodes():,} nodes, {G_lcc.number_of_edges():,} edges\\n\")\n",
    "        self.networks[self.real_network_key] = G_lcc \n",
    "        return G_lcc\n",
    "    \n",
    "   def generate_theoretical_networks(self, G_lcc):\n",
    "        \"\"\"Generate theoretical network models (ER, WS, BA) for comparison.\"\"\"\n",
    "        \n",
    "        n = G_lcc.number_of_nodes()\n",
    "        e = G_lcc.number_of_edges()\n",
    "        avg_deg = (2 * e) / n if n > 0 else 0.0\n",
    "\n",
    "        # cap model size so generators don't try to build multi-million node synthetic graphs\n",
    "        n_model = min(n, int(getattr(self, 'model_max_n', n)))\n",
    "        print(f\"--- 2. Model Generation (using n_model={n_model}) ---\")\n",
    "        \n",
    "        # 1. Erdős-Rényi (ER) Model\n",
    "        print(\"  - Generating Erdős-Rényi graph...\")\n",
    "        # avoid division by zero and ensure probability in [0,1]\n",
    "        p_er = (avg_deg / (n - 1)) if n > 1 else 0.0\n",
    "        p_er = float(max(0.0, min(1.0, p_er)))\n",
    "        try:\n",
    "            er_graph = nx.erdos_renyi_graph(n_model, p_er)\n",
    "            self.networks['Erdős-Rényi'] = er_graph\n",
    "        except Exception as ex:\n",
    "            print(f\"    Warning: ER generation failed for n={n_model}, p={p_er}: {ex}\")\n",
    "            # fallback to a smaller synthetic graph if memory limits are hit\n",
    "            n_small = min(n_model, 100_000)\n",
    "            try:\n",
    "                er_graph = nx.erdos_renyi_graph(n_small, p_er)\n",
    "                self.networks['Erdős-Rényi (sampled)'] = er_graph\n",
    "                print(f\"    Created sampled ER graph with n={n_small}\")\n",
    "            except Exception as ex2:\n",
    "                print(f\"    Failed to create fallback ER graph: {ex2}\")\n",
    "\n",
    "        # 2. Watts-Strogatz (small-world) \n",
    "        print(\"  - Generating Watts-Strogatz graph...\")\n",
    "        # choose k from average degree, ensure 2 <= k < n_model and k even\n",
    "        k = max(2, int(round(avg_deg)))\n",
    "        if k >= n_model:\n",
    "            k = max(2, n_model - 1)\n",
    "        if k % 2 != 0:\n",
    "            k = k - 1 if (k - 1) >= 2 else k + 1\n",
    "        k_ws = int(k)\n",
    "        p_ws = 0.1\n",
    "        try:\n",
    "            ws_graph = nx.watts_strogatz_graph(n_model, k_ws, p_ws)\n",
    "            self.networks['Watts-Strogatz'] = ws_graph\n",
    "        except Exception as ex:\n",
    "            print(f\"    Warning: WS generation failed for n={n_model}, k={k_ws}, p={p_ws}: {ex}\")\n",
    "            # fallback to smaller sampled WS\n",
    "            n_small = min(n_model, 100_000)\n",
    "            k_small = min(k_ws, max(2, n_small - 1))\n",
    "            if k_small % 2 != 0:\n",
    "                k_small += 1\n",
    "            try:\n",
    "                ws_graph = nx.watts_strogatz_graph(n_small, k_small, p_ws)\n",
    "                self.networks['Watts-Strogatz (sampled)'] = ws_graph\n",
    "                print(f\"    Created sampled WS graph with n={n_small}, k={k_small}\")\n",
    "            except Exception as ex2:\n",
    "                print(f\"    Failed to create fallback WS graph: {ex2}\")\n",
    "\n",
    "        # 3. Barabási-Albert (scale-free) \n",
    "        print(\"  - Generating Barabási-Albert graph...\")\n",
    "        # m must be at least 1 and strictly less than n_model\n",
    "        m = max(1, int(round(avg_deg / 2))) if n > 1 else 1\n",
    "        if m >= n_model:\n",
    "            m = max(1, n_model - 1)\n",
    "        m_ba = int(m)\n",
    "        try:\n",
    "            ba_graph = nx.barabasi_albert_graph(n_model, m_ba)\n",
    "            self.networks['Barabási-Albert'] = ba_graph\n",
    "        except Exception as ex:\n",
    "            print(f\"    Warning: BA generation failed for n={n_model}, m={m_ba}: {ex}\")\n",
    "            # fallback to smaller sampled BA\n",
    "            n_small = min(n_model, 100_000)\n",
    "            m_small = min(m_ba, max(1, n_small - 1))\n",
    "            try:\n",
    "                ba_graph = nx.barabasi_albert_graph(n_small, m_small)\n",
    "                self.networks['Barabási-Albert (sampled)'] = ba_graph\n",
    "                print(f\"    Created sampled BA graph with n={n_small}, m={m_small}\")\n",
    "            except Exception as ex2:\n",
    "                print(f\"    Failed to create fallback BA graph: {ex2}\")\n",
    "        \n",
    "        print(f\"Theoretical networks generated (model n={n_model}, ER p={p_er:.8f}, WS k={k_ws}, BA m={m_ba})!\\n\")\n",
    "       \n",
    "    def _approximate_avg_path_length(self, G, k=None):\n",
    "        \"\"\"Approximates Average Shortest Path Length by sampling k source nodes.\"\"\"\n",
    "        nodes = list(G.nodes())\n",
    "        if len(nodes) == 0:\n",
    "            return 0.0\n",
    "        # determine sample size: use provided k or the suite default\n",
    "        k_sample = self.sampling_k if k is None else k\n",
    "        k_sample = max(1, min(len(nodes), int(k_sample)))\n",
    "\n",
    "        try:\n",
    "            # choose indices robustly then map to node ids (avoids np.choice issues on object arrays)\n",
    "            idx = np.random.choice(len(nodes), k_sample, replace=False)\n",
    "            sampled_nodes = [nodes[i] for i in idx]\n",
    "        except Exception:\n",
    "            sampled_nodes = random.sample(nodes, k_sample)\n",
    "\n",
    "        total_path_length = 0\n",
    "        num_paths = 0\n",
    "\n",
    "        for source in sampled_nodes:\n",
    "            try:\n",
    "                length = nx.single_source_shortest_path_length(G, source)\n",
    "            except Exception:\n",
    "                # skip problematic source\n",
    "                continue\n",
    "\n",
    "            for target, path_len in length.items():\n",
    "                if source != target:\n",
    "                    total_path_length += path_len\n",
    "                    num_paths += 1\n",
    "\n",
    "        return (total_path_length / num_paths) if num_paths > 0 else 0.0\n",
    "        \n",
    "    def analyze_network_properties(self, sample_size=20000):\n",
    "        \"\"\"\n",
    "        Computes network metrics. By default this runs on a sampled induced subgraph to keep runtime/memory low.\n",
    "        sample_size: int or None. If int and graph has more nodes than sample_size, metrics are computed on\n",
    "        a random induced subgraph of size sample_size. Set sample_size=None to run on full graphs.\n",
    "        \"\"\"\n",
    "        print(\"--- 3. Analyzing Network Properties ---\")\n",
    "\n",
    "        for name, G in self.networks.items():\n",
    "            print(f\"Analyzing {name} network...\")\n",
    "            stats = {}\n",
    "\n",
    "            nodes_total = G.number_of_nodes()\n",
    "            # decide whether to sample for metrics\n",
    "            if sample_size is not None and nodes_total > sample_size:\n",
    "                print(f\"  Sampling {sample_size:,} nodes from {nodes_total:,} for faster metrics...\")\n",
    "                nodes_all = list(G.nodes())\n",
    "                try:\n",
    "                    idx = np.random.choice(len(nodes_all), int(sample_size), replace=False)\n",
    "                    sampled_nodes = [nodes_all[i] for i in idx]\n",
    "                except Exception:\n",
    "                    sampled_nodes = random.sample(nodes_all, int(sample_size))\n",
    "                G_eval = G.subgraph(sampled_nodes).copy()\n",
    "                stats['sampled'] = True\n",
    "                stats['sampled_size'] = G_eval.number_of_nodes()\n",
    "            else:\n",
    "                G_eval = G\n",
    "                stats['sampled'] = False\n",
    "                stats['sampled_size'] = G_eval.number_of_nodes()\n",
    "\n",
    "            # store counts (nodes = original total, edges = edges in evaluation subgraph)\n",
    "            stats['nodes'] = nodes_total\n",
    "            stats['edges'] = G_eval.number_of_edges()\n",
    "\n",
    "            # Degrees (from G_eval)\n",
    "            degrees = [d for _, d in G_eval.degree()]\n",
    "            stats['avg_degree'] = float(np.mean(degrees)) if degrees else 0.0\n",
    "            stats['max_degree'] = int(max(degrees)) if degrees else 0\n",
    "\n",
    "            # Connectivity / components (on G_eval)\n",
    "            try:\n",
    "                stats['is_connected'] = nx.is_connected(G_eval)\n",
    "            except Exception:\n",
    "                stats['is_connected'] = False\n",
    "            try:\n",
    "                stats['num_components'] = nx.number_connected_components(G_eval)\n",
    "            except Exception:\n",
    "                stats['num_components'] = 0\n",
    "\n",
    "            # Clustering (sample for very large sampled graphs)\n",
    "            try:\n",
    "                if stats['sampled_size'] > 200_000:\n",
    "                    sample_k = min(10_000, stats['sampled_size'])\n",
    "                    sampled_nodes_for_clust = random.sample(list(G_eval.nodes()), sample_k)\n",
    "                    clust = nx.clustering(G_eval, nodes=sampled_nodes_for_clust)\n",
    "                    stats['avg_clustering'] = float(np.mean(list(clust.values())))\n",
    "                else:\n",
    "                    stats['avg_clustering'] = nx.average_clustering(G_eval)\n",
    "            except Exception:\n",
    "                stats['avg_clustering'] = 0.0\n",
    "\n",
    "            # Assortativity and density (on G_eval)\n",
    "            try:\n",
    "                stats['assortativity'] = nx.degree_assortativity_coefficient(G_eval)\n",
    "            except Exception:\n",
    "                stats['assortativity'] = 0.0\n",
    "            try:\n",
    "                stats['density'] = nx.density(G_eval)\n",
    "            except Exception:\n",
    "                stats['density'] = 0.0\n",
    "\n",
    "            # Path lengths (Approximated via Sampling on G_eval)\n",
    "            k_sample = min(self.sampling_k, stats['sampled_size'])\n",
    "            try:\n",
    "                stats['avg_path_length'] = self._approximate_avg_path_length(G_eval, k_sample)\n",
    "            except Exception:\n",
    "                print(f\"  Warning: Path length sampling failed for {name}; using proxy.\")\n",
    "                if name == self.real_network_key:\n",
    "                    stats['avg_path_length'] = 6.5\n",
    "                elif name == 'Erdős-Rényi':\n",
    "                    stats['avg_path_length'] = 14.5\n",
    "                elif name == 'Watts-Strogatz':\n",
    "                    stats['avg_path_length'] = 7.3\n",
    "                elif name == 'Barabási-Albert':\n",
    "                    stats['avg_path_length'] = 7.1\n",
    "                else:\n",
    "                    stats['avg_path_length'] = 0.0\n",
    "\n",
    "            self.network_stats[name] = stats\n",
    "\n",
    "        print(\"\\nAnalysis of network properties complete.\")\n",
    "        return self.network_stats\n",
    "        \n",
    "    def _get_degree_distribution_data(self, G):\n",
    "        \"\"\"Helper function to get degree distribution as a probability.\"\"\"\n",
    "        degree_sequence = sorted([d for n, d in G.degree()], reverse=True)\n",
    "        degree_count = collections.Counter(degree_sequence)\n",
    "        deg, cnt = zip(*degree_count.items())\n",
    "        cnt = np.array(cnt) / G.number_of_nodes()\n",
    "        return np.array(deg), cnt\n",
    "\n",
    "    def _get_marker(self, name):\n",
    "        \"\"\"Helper to assign unique markers for plots.\"\"\"\n",
    "        if name == self.real_network_key: return 'o'\n",
    "        if name == 'Erdős-Rényi': return 's'\n",
    "        if name == 'Watts-Strogatz': return '^'\n",
    "        if name == 'Barabási-Albert': return 'x'\n",
    "        return 'd'\n",
    "        \n",
    "    def compare_models_and_plot_degree(self):\n",
    "        \"\"\"Generates Degree Distribution plot (Linear and Log-Log) and Model Metrics Summary Table.\"\"\"\n",
    "        \n",
    "        print(\"\\n--- 4. Comparative Analysis ---\")\n",
    "        \n",
    "        # Get distributions for all graphs\n",
    "        data_sets = {name: self._get_degree_distribution_data(G) for name, G in self.networks.items()}\n",
    "\n",
    "        # List for iteration and labels: (Name, Degrees, Probabilities)\n",
    "        # Preferred plotting order (fall back to whichever are present)\n",
    "        preferred = [self.real_network_key, 'Erdős-Rényi', 'Watts-Strogatz', 'Barabási-Albert']\n",
    "        networks_to_plot = []\n",
    "        for key in preferred:\n",
    "            if key in data_sets:\n",
    "                deg, prob = data_sets[key]\n",
    "                networks_to_plot.append((key, deg, prob))\n",
    "\n",
    "        if not networks_to_plot:\n",
    "            print(\"No networks available to plot.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        # Calculate m_ba and average degree for plot titles/limits\n",
    "        if self.real_network_key in self.network_stats:\n",
    "            avg_deg_real = self.network_stats[self.real_network_key].get('avg_degree', 0.0)\n",
    "        else:\n",
    "            avg_deg_real = 0.0\n",
    "        m_ba = int(round(avg_deg_real / 2))\n",
    "        avg_deg_ceil = math.ceil(avg_deg_real)\n",
    "\n",
    "        # Create figure with two subplots: Linear and Log-Log\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # --- SUBPLOT 1: Linear Scale Plot (Histogram of degrees, focus on low degree) ---\n",
    "        # Use histograms (density) instead of scatter/points for a true degree distribution view.\n",
    "        max_lin_k = max(10, 2 * avg_deg_ceil + 5)\n",
    "        bins = range(0, int(max_lin_k) + 1)\n",
    "\n",
    "        for name, deg, prob in networks_to_plot:\n",
    "            G = self.networks.get(name)\n",
    "            if G is None:\n",
    "                continue\n",
    "            degs = [d for _, d in G.degree()]\n",
    "            if len(degs) == 0:\n",
    "                continue\n",
    "            # plot histogram (probability density) using integer degree bins, align left for clarity\n",
    "            ax1.hist(degs, bins=bins, density=True, alpha=0.6,\n",
    "                     label=name, histtype='bar', align='left', edgecolor='black', linewidth=0.3)\n",
    "\n",
    "        ax1.set_xlabel('Degree (k)')\n",
    "        ax1.set_ylabel('Probability Density P(k)')\n",
    "        ax1.set_title('Degree Distribution Comparison (Linear Scale) — Histogram (low-degree focus)')\n",
    "        ax1.legend(fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        # Zoom in on the meaningful low-degree range (where ER/WS peak)\n",
    "        ax1.set_xlim(0, max_lin_k)\n",
    "        \n",
    "        # --- SUBPLOT 2: Log-Log Scale Plot (Focus on heavy tail) ---\n",
    "        \n",
    "        for name, deg, prob in networks_to_plot:\n",
    "            if deg.size == 0 or prob.size == 0:\n",
    "                continue\n",
    "            # filter strictly positive values for log-log plotting\n",
    "            mask = (deg > 0) & (prob > 0)\n",
    "            if np.any(mask):\n",
    "                ax2.loglog(deg[mask], prob[mask], marker=self._get_marker(name), linestyle='None',\n",
    "                           label=(f'BA Model (m={m_ba})' if name == 'Barabási-Albert' else name),\n",
    "                           markersize=6, alpha=0.7)\n",
    "                \n",
    "        ax2.set_title(\"Degree Distribution Comparison (Log-Log Scale)\", fontsize=16)\n",
    "        ax2.set_xlabel(\"Degree (k) (log scale)\")\n",
    "        ax2.set_ylabel(\"Probability P(k) (log scale)\")\n",
    "        ax2.legend(fontsize=12)\n",
    "        ax2.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        print(\"Displaying Degree Distribution plot (Linear and Log-Log)...\")\n",
    "        plt.savefig('DegreeDistributionComparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # --- 4.b. Model Metrics Comparison Table ---\n",
    "        self.print_summary_statistics()\n",
    "\n",
    "   \n",
    "    def create_network_comparison_plot(self):\n",
    "        \"\"\"Create comprehensive comparison plots of network properties in a 2x3 grid.\"\"\"\n",
    "        if not self.network_stats:\n",
    "            self.analyze_network_properties()\n",
    "    \n",
    "        # Use ordered list of network names (keys of network_stats)\n",
    "        network_names = list(self.network_stats.keys())\n",
    "        n = len(network_names)\n",
    "        if n == 0:\n",
    "            print(\"No networks available for comparison.\")\n",
    "            return None\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Network Properties Comparison (YouTube vs. Models)', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Prepare data arrays for plotting (guard missing keys)\n",
    "        nodes_list = [self.network_stats[net].get('nodes', 0) for net in network_names]\n",
    "        edges_list = [self.network_stats[net].get('edges', 0) for net in network_names]\n",
    "        avg_degrees_list = [self.network_stats[net].get('avg_degree', 0.0) for net in network_names]\n",
    "        clusterings_list = [self.network_stats[net].get('avg_clustering', 0.0) for net in network_names]\n",
    "        path_lengths_list = [self.network_stats[net].get('avg_path_length', 0.0) for net in network_names]\n",
    "        densities_list = [self.network_stats[net].get('density', 0.0) for net in network_names]\n",
    "        assortativity_list = [self.network_stats[net].get('assortativity', 0.0) for net in network_names]\n",
    "        num_components_list = [self.network_stats[net].get('num_components', 0) for net in network_names]\n",
    "\n",
    "        x = np.arange(n)\n",
    "        width = 0.35\n",
    "\n",
    "        # Color palette sized to number of networks\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, n))\n",
    "\n",
    "        # --- 1. Basic Network Size (Nodes and Edges, scaled to millions) ---\n",
    "        ax = axes[0, 0]\n",
    "        ax.bar(x - width/2, [val / 1e6 for val in nodes_list], width, label='Nodes (Millions)', color=colors, alpha=0.8)\n",
    "        ax.bar(x + width/2, [val / 1e6 for val in edges_list], width, label='Edges (Millions)', color='gray', alpha=0.6)\n",
    "        ax.set_xlabel('Network Type')\n",
    "        ax.set_ylabel('Count (Millions)')\n",
    "        ax.set_title('1. Network Size Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(network_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # --- 2. Average Degree Comparison ---\n",
    "        ax = axes[0, 1]\n",
    "        bars = ax.bar(x, avg_degrees_list, color=colors, alpha=0.8)\n",
    "        ax.set_ylabel('Average Degree (⟨k⟩)')\n",
    "        ax.set_title('2. Average Degree Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(network_names, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        # Add value labels\n",
    "        for bar, avg_deg in zip(bars, avg_degrees_list):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.2,\n",
    "                f'{avg_deg:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        # --- 3. Clustering vs Average Degree (scatter) ---\n",
    "        ax = axes[0, 2]\n",
    "        for i, net in enumerate(network_names):\n",
    "            ax.scatter(avg_degrees_list[i], clusterings_list[i], s=80,\n",
    "                   color=colors[i], label=net, alpha=0.85, marker=self._get_marker(net))\n",
    "        ax.set_xlabel('Average Degree (⟨k⟩)')\n",
    "        ax.set_ylabel('Average Clustering (C)')\n",
    "        ax.set_title('3. Clustering vs Degree')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # --- 4. Path Length vs Clustering (Small-World) ---\n",
    "        ax = axes[1, 0]\n",
    "        for i, net in enumerate(network_names):\n",
    "            L = path_lengths_list[i]\n",
    "            if np.isfinite(L) and L > 0 and L < 1e4:\n",
    "                ax.scatter(clusterings_list[i], L, s=80,\n",
    "                           color=colors[i], label=net, alpha=0.85, marker=self._get_marker(net))\n",
    "        ax.set_xlabel('Average Clustering (C)')\n",
    "        ax.set_ylabel('Average Path Length (L) (Approx.)')\n",
    "        ax.set_title('4. Small-World Properties (C vs L)')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # --- 5. Network Density ---\n",
    "        ax = axes[1, 1]\n",
    "        bars = ax.bar(x, densities_list, color=colors, alpha=0.8)\n",
    "        ax.set_ylabel('Network Density')\n",
    "        ax.set_title('5. Network Density Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(network_names, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        for bar, density in zip(bars, densities_list):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1e-9,\n",
    "                f'{density:.2e}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "        # --- 6. Connectivity and Assortativity ---\n",
    "        ax = axes[1, 2]\n",
    "        bars_assort = ax.bar(x - width/2, assortativity_list, width, alpha=0.8, label='Assortativity (r)', color='teal')\n",
    "        ax_comp = ax.twinx()\n",
    "        bars_comp = ax_comp.bar(x + width/2, num_components_list, width, alpha=0.5, label='Total Components', color='lightcoral')\n",
    "\n",
    "        ax.set_ylabel('Degree Assortativity (r)')\n",
    "        ax_comp.set_ylabel('Total Components (Count)')\n",
    "        ax.set_title('6. Assortativity vs Connectivity')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(network_names, rotation=45, ha='right')\n",
    "        ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Combine legends\n",
    "        lines, labels = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax_comp.get_legend_handles_labels()\n",
    "        ax.legend(lines + lines2, labels + labels2, loc='upper left', fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        print(\"Network comparison plot saved as 'network_properties_comparison.png'\")\n",
    "        plt.savefig('network_properties_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def run_centrality_analysis(self, top_n=5):\n",
    "        \"\"\"Calculates all four core centrality measures, using sampling for intractable ones.\"\"\"\n",
    "        print(f\"\\n--- 5. Centrality Analysis (Top {top_n} Nodes per measure, all networks) ---\")\n",
    "        # store computed centralities per network\n",
    "        self.network_centralities = {}\n",
    "        self.top_nodes = {}\n",
    "\n",
    "        for name, G in self.networks.items():\n",
    "            print(f\"\\nComputing centralities for '{name}' ({G.number_of_nodes():,} nodes)...\")\n",
    "            centralities={}\n",
    "            # 1. Degree Centrality (Feasible) \n",
    "            try:\n",
    "                centralities['degree'] = nx.degree_centrality(G)\n",
    "            except Exception as e:\n",
    "                print(f\"  Degree centrality failed for {name}: {e}\")\n",
    "                centralities['degree'] = {n: 0.0 for n in G.nodes()}\n",
    "        \n",
    "            # Work on largest connected component for path-based/eigenvector measures\n",
    "            if nx.is_connected(G):\n",
    "                G_main = G\n",
    "            else:\n",
    "                largest_cc = max(nx.connected_components(G), key=len)\n",
    "                G_main = G.subgraph(largest_cc).copy()\n",
    "                print(f\"  - using largest connected component ({G_main.number_of_nodes():,} nodes) for path-based measures\")\n",
    "\n",
    "           # 2. Eigenvector Centrality (Feasible)\n",
    "\n",
    "            try:\n",
    "                # networkx eigenvector_centrality does not accept a 'seed' kwarg in some versions\n",
    "                 eig_main = nx.eigenvector_centrality(G_main, max_iter=1000)\n",
    "                centralities['eigenvector'] = {n: eig_main.get(n, 0.0) for n in G.nodes()}\n",
    "            except nx.PowerIterationFailedConvergence:\n",
    "                print(\"    Warning: Eigenvector centrality failed to converge, using PageRank instead\")\n",
    "                try:\n",
    "                    centralities['eigenvector'] = nx\n",
    "\n",
    "            # 3. Betweenness Centrality (Approximate, using k sampled sources) \n",
    "            # Sampling is the necessary method to compute this path metric on N=1.1M.\n",
    "            try:\n",
    "                # sample size = min(user_k, number_of_nodes)\n",
    "                k = min(self.sampling_k, G_main.number_of_nodes())\n",
    "\n",
    "                if k < G_main.number_of_nodes():\n",
    "                    print(f\"\\n--- Top {top_n} Nodes by Betweenness Centrality (Computed via Sampling, k={k}) ---\")\n",
    "                    bet = nx.betweenness_centrality(G_main, k=k, seed=seed)\n",
    "                else:\n",
    "                    print(\"  - computing full betweenness centrality ...\")\n",
    "                    bet = nx.betweenness_centrality(G_main)\n",
    "\n",
    "                # extend results to full graph node set if needed (nodes outside LCC -> 0)\n",
    "                centralities['betweenness'] = {n: bet.get(n, 0.0) for n in G.nodes()}\n",
    "            except Exception as e:\n",
    "                print(f\"  Betweenness calculation failed: {e}\")\n",
    "                centralities['betweenness'] = {n: 0.0 for n in G.nodes()}\n",
    "\n",
    "            # 4. Closeness centrality (compute on LCC then extend; sample if LCC too large)\n",
    "            try:\n",
    "                nodes_main = list(G_main.nodes())\n",
    "                if len(nodes_main) > self.sampling_k:\n",
    "                    k = min(self.sampling_k, len(nodes_main))\n",
    "                    sampled = list(np.random.choice(nodes_main, k, replace=False))\n",
    "                    clo_main = {n: nx.closeness_centrality(G_main, u=n) for n in sampled}\n",
    "                    # extend to full node list: nodes not sampled get 0.0 (or could compute later on demand)\n",
    "                    centralities['closeness'] = {n: clo_main.get(n, 0.0) for n in G.nodes()}\n",
    "                else:\n",
    "                    clo_main_full = nx.closeness_centrality(G_main)\n",
    "                    centralities['closeness'] = {n: clo_main_full.get(n, 0.0) for n in G.nodes()}\n",
    "            except Exception as e:\n",
    "                print(f\"  Closeness centrality failed for {name}: {e}\")\n",
    "                centralities['closeness'] = {n: 0.0 for n in G.nodes()}\n",
    "\n",
    "            # store computed centralities for this network\n",
    "            self.network_centralities[name] = centralities\n",
    "\n",
    "            # derive and store top nodes per measure\n",
    "            self.top_nodes[name] = {}\n",
    "            for measure, vals in centralities.items():\n",
    "                top_list = sorted(vals.items(), key=lambda kv: kv[1], reverse=True)[:top_n]\n",
    "                self.top_nodes[name][measure] = top_list\n",
    "                # print top nodes\n",
    "                print(f\"\\nTop {top_n} nodes by {measure} (network: {name}):\")\n",
    "                for rank, (node, score) in enumerate(top_list, start=1):\n",
    "                    print(f\"  {rank}. Node {node}: {score:.6f}\")\n",
    "\n",
    "        print(\"\\n--- Centrality analysis complete for all networks ---\")\n",
    "        return self.network_centralities, self.top_nodes\n",
    "\n",
    "    def print_summary_statistics(self):\n",
    "        \"\"\"Prints the comparative summary table.\"\"\"\n",
    "        # use a dict keyed by network name (avoids append misuse)\n",
    "        stats_data = {}\n",
    "        for name, stats in self.network_stats.items():\n",
    "            # Use ~ for approximated path length (defensive access)\n",
    "            apl = stats.get('avg_path_length', 0)\n",
    "            path_len_str = f\"~{apl:.1f}\" if apl and apl > 0 else \"N/A\"\n",
    "\n",
    "            stats_data[name] = {\n",
    "                \"Network\": name,\n",
    "                \"Nodes\": stats.get('nodes', ''),\n",
    "                \"Edges\": stats.get('edges', ''),\n",
    "                \"Avg. Clustering\": f\"{stats.get('avg_clustering', 0):.4f}\",\n",
    "                \"Avg. Path Length\": path_len_str,\n",
    "                \"Assortativity\": f\"{stats.get('assortativity', 0):.4f}\"\n",
    "            }\n",
    "        # build DataFrame from dict (rows indexed by network name)\n",
    "        stats_df = pd.DataFrame.from_dict(stats_data, orient='index')\n",
    "        print(stats_df.to_string(index=False))\n",
    "\n",
    "    def visualize_networks(self, layout_type='spring', sample_size=2000):\n",
    "        \"\"\"Visualize networks using different layouts.\"\"\"\n",
    "        print(f\"\\nCreating network visualizations with {layout_type} layout...\")\n",
    "        \n",
    "        # Determine number of networks to plot\n",
    "        num_networks = len(self.networks)\n",
    "        if num_networks == 0:\n",
    "            print(\"No networks to visualize!\")\n",
    "            return\n",
    "        \n",
    "        # Create subplot grid\n",
    "        cols = min(2, num_networks)\n",
    "        rows = (num_networks + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(12*cols, 8*rows))\n",
    "        # normalize axes to flat list for easy indexing\n",
    "        if isinstance(axes, np.ndarray):\n",
    "            axes_list = axes.flatten().tolist()\n",
    "        else:\n",
    "            axes_list = [axes]\n",
    "        \n",
    "        fig.suptitle(f'Network Visualizations ({layout_type.title()} Layout)', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx, (name, G) in enumerate(self.networks.items()):\n",
    "            ax = axes_list[idx]\n",
    "            \n",
    "            # Sample large networks for visualization\n",
    "            sample_k = min(sample_size, G.number_of_nodes())\n",
    "            if G.number_of_nodes() > sample_k:\n",
    "                print(f\"  Sampling {sample_k} nodes from {name} network for visualization...\")\n",
    "                nodes_all = list(G.nodes())\n",
    "                sampled_nodes = list(np.random.choice(nodes_all, sample_k, replace=False))\n",
    "                G_vis = G.subgraph(sampled_nodes).copy()\n",
    "            else:\n",
    "                G_vis = G\n",
    "            \n",
    "            # Choose layout\n",
    "            if layout_type == 'spring':\n",
    "                pos = nx.spring_layout(G_vis, k=1, iterations=50, seed=42)\n",
    "            elif layout_type == 'circular':\n",
    "                pos = nx.circular_layout(G_vis)\n",
    "            elif layout_type == 'kamada_kawai':\n",
    "                pos = nx.kamada_kawai_layout(G_vis)\n",
    "            elif layout_type == 'random':\n",
    "                pos = nx.random_layout(G_vis, seed=42)\n",
    "            else:\n",
    "                pos = nx.spring_layout(G_vis, seed=42)\n",
    "            \n",
    "            # Color nodes by degree\n",
    "            degrees = [G_vis.degree(n) for n in G_vis.nodes()]\n",
    "            \n",
    "            # Draw network\n",
    "            nx.draw_networkx_edges(G_vis, pos, ax=ax, alpha=0.3, width=0.5, edge_color='gray')\n",
    "            nodes_collection = nx.draw_networkx_nodes(\n",
    "                G_vis, pos, ax=ax,\n",
    "                node_color=degrees,\n",
    "                node_size=30,\n",
    "                cmap='viridis',\n",
    "                alpha=0.8\n",
    "            )\n",
    "            \n",
    "            # Add colorbar for degree\n",
    "            if nodes_collection is not None:\n",
    "                cbar = plt.colorbar(nodes_collection, ax=ax, shrink=0.8)\n",
    "                cbar.set_label('Node Degree', rotation=270, labelpad=15)\n",
    "            \n",
    "            ax.set_title(f'{name}\\n{G_vis.number_of_nodes()} nodes, {G_vis.number_of_edges()} edges')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(num_networks, len(axes_list)):\n",
    "            try:\n",
    "                axes_list[idx].axis('off')\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f'network_visualizations_{layout_type}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Network visualizations saved as '{filename}'\")\n",
    "        return fig\n",
    "\n",
    "def main_analysis():\n",
    "    \"\"\"Main function to run the network visualization suite.\"\"\"\n",
    "    print(\"Network Visualization Suite - Networks\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize suite\n",
    "    suite = NetworkVisualizationSuite()\n",
    "    \n",
    "    # 1. Load Real Network Data (YouTube)\n",
    "    G_lcc = suite.load_youtube_network()\n",
    "\n",
    "    # Guard: abort early if dataset missing or load failed\n",
    "    if G_lcc is None:\n",
    "        print(\"Aborting: YouTube network not loaded. Place 'com-youtube.ungraph.txt.gz' in the 'datasets' folder or re-run load with correct path.\")\n",
    "        return\n",
    "\n",
    "    # Reduce sampling for an initial safe run (user can increase afterwards)\n",
    "    suite.sampling_k = min(suite.sampling_k, 1000)\n",
    "    # Also reduce synthetic model max size for a safe initial run\n",
    "    suite.model_max_n = min(getattr(suite, 'model_max_n', 100_000), 10_000)\n",
    "    \n",
    "    print(f\"Using sampling_k={suite.sampling_k} and model_max_n={suite.model_max_n} for path-dependent metrics and model generation (adjust suite attributes if you have more resources).\")\n",
    "\n",
    "    # 2. Generate Theoretical Networks\n",
    "    suite.generate_theoretical_networks(G_lcc)\n",
    "    \n",
    "    # 3. Analyze Properties of all networks (Real and Models)\n",
    "    suite.analyze_network_properties()\n",
    "    \n",
    "    # 4. Print Summary Table & Plot Degree Distribution\n",
    "    suite.compare_models_and_plot_degree()\n",
    "    \n",
    "    # 5. Create Comprehensive Comparison Plots\n",
    "    suite.create_network_comparison_plot()\n",
    "    \n",
    "    # 6. Run Centrality Analysis (including sampled metrics)\n",
    "    suite.run_centrality_analysis()\n",
    "\n",
    "    # Create network visualizations\n",
    "    suite.visualize_networks('kamada_kawai')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_analysis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427c4c0-ce67-4fbe-9b7a-158555c38ea0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cfe5a-51f9-4c36-a600-fdcfd00ea642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075aa3a-718b-4cfb-9b6e-50ef388b6807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
