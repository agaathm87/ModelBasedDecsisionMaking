{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee60c10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Facebook Network Explorer\n",
    "=========================\n",
    "\n",
    "Detailed analysis and visualization of the Facebook social network dataset.\n",
    "This program provides in-depth exploration of real-world network properties.\n",
    "\n",
    "Features:\n",
    "- Detailed Facebook network analysis\n",
    "- Community detection\n",
    "- Centrality analysis\n",
    "- Degree distribution analysis\n",
    "- Interactive subgraph exploration\n",
    "- Comparison with random networks\n",
    "\n",
    "Author: Michael Lees\n",
    "Date: 2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de78d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78bd76",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49be27c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FacebookNetworkExplorer:\n",
    "    \"\"\"Detailed Facebook network analysis and exploration.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"datasets\"):\n",
    "        \"\"\"Initialize the explorer with data directory.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.facebook_graph = None\n",
    "        self.communities = None\n",
    "        self.centralities = {}\n",
    "        \n",
    "    def load_facebook_network(self, filename=\"facebook_combined.txt.gz\"):\n",
    "        \"\"\"Load and preprocess Facebook social network.\"\"\"\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Error: {filepath} not found!\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"Loading Facebook network from {filename}...\")\n",
    "        \n",
    "        # Load the network\n",
    "        self.facebook_graph = nx.Graph()\n",
    "        \n",
    "        with gzip.open(filepath, 'rt') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    u, v = map(int, line.strip().split())\n",
    "                    self.facebook_graph.add_edge(u, v)\n",
    "        \n",
    "        print(f\"✓ Loaded Facebook network:\")\n",
    "        print(f\"  • {self.facebook_graph.number_of_nodes():,} nodes (users)\")\n",
    "        print(f\"  • {self.facebook_graph.number_of_edges():,} edges (friendships)\")\n",
    "        print(f\"  • Average degree: {2*self.facebook_graph.number_of_edges()/self.facebook_graph.number_of_nodes():.2f}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def analyze_basic_properties(self):\n",
    "        \"\"\"Analyze basic network properties.\"\"\"\n",
    "        if self.facebook_graph is None:\n",
    "            print(\"Please load the Facebook network first!\")\n",
    "            return\n",
    "        \n",
    "        G = self.facebook_graph\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BASIC NETWORK PROPERTIES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Basic stats\n",
    "        n_nodes = G.number_of_nodes()\n",
    "        n_edges = G.number_of_edges()\n",
    "        density = nx.density(G)\n",
    "        \n",
    "        print(f\"Nodes (Users): {n_nodes:,}\")\n",
    "        print(f\"Edges (Friendships): {n_edges:,}\")\n",
    "        print(f\"Network Density: {density:.6f}\")\n",
    "        print(f\"Maximum Possible Edges: {n_nodes*(n_nodes-1)//2:,}\")\n",
    "        \n",
    "        # Degree statistics\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        print(f\"\\nDegree Statistics:\")\n",
    "        print(f\"  • Average degree: {np.mean(degrees):.2f}\")\n",
    "        print(f\"  • Median degree: {np.median(degrees):.2f}\")\n",
    "        print(f\"  • Max degree: {max(degrees)}\")\n",
    "        print(f\"  • Min degree: {min(degrees)}\")\n",
    "        print(f\"  • Degree std: {np.std(degrees):.2f}\")\n",
    "        \n",
    "        # Connectivity\n",
    "        is_connected = nx.is_connected(G)\n",
    "        n_components = nx.number_connected_components(G)\n",
    "        \n",
    "        print(f\"\\nConnectivity:\")\n",
    "        print(f\"  • Is connected: {is_connected}\")\n",
    "        print(f\"  • Number of components: {n_components}\")\n",
    "        \n",
    "        if not is_connected:\n",
    "            # Analyze largest component\n",
    "            largest_cc = max(nx.connected_components(G), key=len)\n",
    "            largest_cc_size = len(largest_cc)\n",
    "            print(f\"  • Largest component size: {largest_cc_size:,} ({100*largest_cc_size/n_nodes:.1f}%)\")\n",
    "            \n",
    "            # Use largest component for path analysis\n",
    "            G_main = G.subgraph(largest_cc)\n",
    "        else:\n",
    "            G_main = G\n",
    "        \n",
    "        # Path lengths (on main component)\n",
    "        if G_main.number_of_nodes() > 1:\n",
    "            avg_path_length = nx.average_shortest_path_length(G_main)\n",
    "            diameter = nx.diameter(G_main)\n",
    "            print(f\"  • Average path length: {avg_path_length:.2f}\")\n",
    "            print(f\"  • Diameter: {diameter}\")\n",
    "        \n",
    "        # Clustering\n",
    "        avg_clustering = nx.average_clustering(G)\n",
    "        global_clustering = nx.transitivity(G)\n",
    "        \n",
    "        print(f\"\\nClustering:\")\n",
    "        print(f\"  • Average clustering coefficient: {avg_clustering:.4f}\")\n",
    "        print(f\"  • Global clustering coefficient: {global_clustering:.4f}\")\n",
    "        \n",
    "        # Compare with random network\n",
    "        p_random = density\n",
    "        expected_clustering_random = p_random\n",
    "        print(f\"  • Expected clustering (random): {expected_clustering_random:.6f}\")\n",
    "        print(f\"  • Clustering enhancement: {avg_clustering/expected_clustering_random:.1f}x higher than random\")\n",
    "    \n",
    "    def analyze_degree_distribution(self):\n",
    "        \"\"\"Analyze and visualize degree distribution.\"\"\"\n",
    "        if self.facebook_graph is None:\n",
    "            return\n",
    "        \n",
    "        G = self.facebook_graph\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        degree_counts = Counter(degrees)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"DEGREE DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Create degree distribution plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Facebook Network: Degree Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Histogram\n",
    "        ax = axes[0, 0]\n",
    "        ax.hist(degrees, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.set_xlabel('Degree')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Degree Distribution (Linear Scale)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        ax.axvline(np.mean(degrees), color='red', linestyle='--', label=f'Mean: {np.mean(degrees):.1f}')\n",
    "        ax.axvline(np.median(degrees), color='orange', linestyle='--', label=f'Median: {np.median(degrees):.1f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # 2. Log-log plot\n",
    "        ax = axes[0, 1]\n",
    "        degrees_unique = sorted(degree_counts.keys())\n",
    "        counts = [degree_counts[d] for d in degrees_unique]\n",
    "        \n",
    "        ax.loglog(degrees_unique, counts, 'bo-', alpha=0.7, markersize=4)\n",
    "        ax.set_xlabel('Degree (log scale)')\n",
    "        ax.set_ylabel('Frequency (log scale)')\n",
    "        ax.set_title('Degree Distribution (Log-Log Scale)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Fit power law\n",
    "        log_degrees = np.log(degrees_unique[1:])  # Exclude degree 0\n",
    "        log_counts = np.log(counts[1:])\n",
    "        slope, intercept = np.polyfit(log_degrees, log_counts, 1)\n",
    "        \n",
    "        ax.plot(degrees_unique[1:], np.exp(intercept) * np.array(degrees_unique[1:])**slope, \n",
    "                'r--', label=f'Power law fit: γ ≈ {-slope:.2f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # 3. Cumulative distribution\n",
    "        ax = axes[1, 0]\n",
    "        degrees_sorted = np.sort(degrees)\n",
    "        cumulative = 1 - np.arange(1, len(degrees_sorted) + 1) / len(degrees_sorted)\n",
    "        \n",
    "        ax.loglog(degrees_sorted, cumulative, 'go-', alpha=0.7, markersize=2)\n",
    "        ax.set_xlabel('Degree (log scale)')\n",
    "        ax.set_ylabel('P(Degree ≥ k)')\n",
    "        ax.set_title('Cumulative Degree Distribution')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Degree vs Local Clustering\n",
    "        ax = axes[1, 1]\n",
    "        clustering_dict = nx.clustering(G)\n",
    "        node_degrees = dict(G.degree())\n",
    "        \n",
    "        # Sample for visualization if too many points\n",
    "        nodes_sample = list(G.nodes())\n",
    "        if len(nodes_sample) > 1000:\n",
    "            nodes_sample = np.random.choice(nodes_sample, 1000, replace=False)\n",
    "        \n",
    "        degrees_sample = [node_degrees[n] for n in nodes_sample]\n",
    "        clustering_sample = [clustering_dict[n] for n in nodes_sample]\n",
    "        \n",
    "        ax.scatter(degrees_sample, clustering_sample, alpha=0.5, s=10)\n",
    "        ax.set_xlabel('Node Degree')\n",
    "        ax.set_ylabel('Local Clustering Coefficient')\n",
    "        ax.set_title('Degree vs Local Clustering')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(degrees_sample) > 10:\n",
    "            z = np.polyfit(degrees_sample, clustering_sample, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax.plot(sorted(degrees_sample), p(sorted(degrees_sample)), \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('facebook_degree_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Degree distribution analysis saved as 'facebook_degree_analysis.png'\")\n",
    "        \n",
    "        # Print key statistics\n",
    "        print(f\"\\nKey Findings:\")\n",
    "        print(f\"  • Power law exponent (γ): ~{-slope:.2f}\")\n",
    "        print(f\"  • Most connected user has {max(degrees)} friends\")\n",
    "        print(f\"  • {sum(1 for d in degrees if d > 100)} users have >100 friends\")\n",
    "        print(f\"  • {sum(1 for d in degrees if d == 1)} users have only 1 friend\")\n",
    "    \n",
    "    def calculate_centralities(self, sample_size=1000):\n",
    "        \"\"\"Calculate various centrality measures.\"\"\"\n",
    "        if self.facebook_graph is None:\n",
    "            return\n",
    "        \n",
    "        G = self.facebook_graph\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"CENTRALITY ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # For large networks, work with largest connected component\n",
    "        if not nx.is_connected(G):\n",
    "            largest_cc = max(nx.connected_components(G), key=len)\n",
    "            G_main = G.subgraph(largest_cc)\n",
    "            print(f\"Working with largest connected component: {G_main.number_of_nodes():,} nodes\")\n",
    "        else:\n",
    "            G_main = G\n",
    "        \n",
    "        # Sample for computational efficiency\n",
    "        if G_main.number_of_nodes() > sample_size:\n",
    "            nodes_sample = np.random.choice(list(G_main.nodes()), sample_size, replace=False)\n",
    "            G_sample = G_main.subgraph(nodes_sample)\n",
    "            print(f\"Sampling {sample_size} nodes for centrality calculations...\")\n",
    "        else:\n",
    "            G_sample = G_main\n",
    "            nodes_sample = list(G_sample.nodes())\n",
    "        \n",
    "        print(\"Calculating centrality measures...\")\n",
    "        \n",
    "        # Degree centrality\n",
    "        print(\"  • Degree centrality...\")\n",
    "        degree_cent = nx.degree_centrality(G_sample)\n",
    "        \n",
    "        # Betweenness centrality\n",
    "        print(\"  • Betweenness centrality...\")\n",
    "        betweenness_cent = nx.betweenness_centrality(G_sample, k=min(100, len(nodes_sample)))\n",
    "        \n",
    "        # Closeness centrality\n",
    "        print(\"  • Closeness centrality...\")\n",
    "        closeness_cent = nx.closeness_centrality(G_sample)\n",
    "        \n",
    "        # Eigenvector centrality\n",
    "        print(\"  • Eigenvector centrality...\")\n",
    "        try:\n",
    "            eigenvector_cent = nx.eigenvector_centrality(G_sample, max_iter=1000)\n",
    "        except:\n",
    "            eigenvector_cent = {n: 0 for n in G_sample.nodes()}\n",
    "            print(\"    (Eigenvector centrality calculation failed)\")\n",
    "        \n",
    "        self.centralities = {\n",
    "            'degree': degree_cent,\n",
    "            'betweenness': betweenness_cent,\n",
    "            'closeness': closeness_cent,\n",
    "            'eigenvector': eigenvector_cent\n",
    "        }\n",
    "        \n",
    "        # Create centrality comparison plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Facebook Network: Centrality Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        centrality_names = ['Degree', 'Betweenness', 'Closeness', 'Eigenvector']\n",
    "        centrality_keys = ['degree', 'betweenness', 'closeness', 'eigenvector']\n",
    "        \n",
    "        for i, (name, key) in enumerate(zip(centrality_names, centrality_keys)):\n",
    "            ax = axes[i//2, i%2]\n",
    "            values = list(self.centralities[key].values())\n",
    "            \n",
    "            ax.hist(values, bins=30, alpha=0.7, color=plt.cm.Set3(i), edgecolor='black')\n",
    "            ax.set_xlabel(f'{name} Centrality')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'{name} Centrality Distribution')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_val = np.mean(values)\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', \n",
    "                      label=f'Mean: {mean_val:.4f}')\n",
    "            ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('facebook_centrality_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Centrality analysis saved as 'facebook_centrality_analysis.png'\")\n",
    "        \n",
    "        # Print top nodes for each centrality\n",
    "        print(f\"\\nTop 5 nodes by centrality:\")\n",
    "        for name, key in zip(centrality_names, centrality_keys):\n",
    "            top_nodes = sorted(self.centralities[key].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"\\n{name} Centrality:\")\n",
    "            for rank, (node, score) in enumerate(top_nodes, 1):\n",
    "                print(f\"  {rank}. Node {node}: {score:.4f}\")\n",
    "    \n",
    "    def visualize_network_sample(self, sample_size=100, layout='spring'):\n",
    "        \"\"\"Visualize a sample of the Facebook network.\"\"\"\n",
    "        if self.facebook_graph is None:\n",
    "            return\n",
    "        \n",
    "        G = self.facebook_graph\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"NETWORK VISUALIZATION (Sample of {sample_size} nodes)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Sample nodes\n",
    "        nodes_sample = np.random.choice(list(G.nodes()), \n",
    "                                       min(sample_size, G.number_of_nodes()), \n",
    "                                       replace=False)\n",
    "        G_sample = G.subgraph(nodes_sample)\n",
    "        \n",
    "        print(f\"Sampled subgraph: {G_sample.number_of_nodes()} nodes, {G_sample.number_of_edges()} edges\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        fig.suptitle(f'Facebook Network Sample Visualization ({sample_size} nodes)', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Choose layout\n",
    "        if layout == 'spring':\n",
    "            pos = nx.spring_layout(G_sample, k=2, iterations=50, seed=42)\n",
    "        elif layout == 'kamada_kawai':\n",
    "            pos = nx.kamada_kawai_layout(G_sample)\n",
    "        else:\n",
    "            pos = nx.spring_layout(G_sample, seed=42)\n",
    "        \n",
    "        # Plot 1: Colored by degree\n",
    "        ax = axes[0]\n",
    "        degrees = [G_sample.degree(n) for n in G_sample.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_edges(G_sample, pos, ax=ax, alpha=0.3, width=0.5, edge_color='lightgray')\n",
    "        nodes = nx.draw_networkx_nodes(G_sample, pos, ax=ax,\n",
    "                                     node_color=degrees,\n",
    "                                     node_size=[50 + 10*d for d in degrees],\n",
    "                                     cmap='viridis',\n",
    "                                     alpha=0.8)\n",
    "        \n",
    "        if nodes is not None:\n",
    "            cbar = plt.colorbar(nodes, ax=ax, shrink=0.8)\n",
    "            cbar.set_label('Node Degree', rotation=270, labelpad=15)\n",
    "        \n",
    "        ax.set_title('Nodes Colored by Degree')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Plot 2: Colored by clustering\n",
    "        ax = axes[1]\n",
    "        clustering_dict = nx.clustering(G_sample)\n",
    "        clustering_values = [clustering_dict[n] for n in G_sample.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_edges(G_sample, pos, ax=ax, alpha=0.3, width=0.5, edge_color='lightgray')\n",
    "        nodes = nx.draw_networkx_nodes(G_sample, pos, ax=ax,\n",
    "                                     node_color=clustering_values,\n",
    "                                     node_size=[50 + 10*d for d in degrees],\n",
    "                                     cmap='plasma',\n",
    "                                     alpha=0.8)\n",
    "        \n",
    "        if nodes is not None:\n",
    "            cbar = plt.colorbar(nodes, ax=ax, shrink=0.8)\n",
    "            cbar.set_label('Local Clustering', rotation=270, labelpad=15)\n",
    "        \n",
    "        ax.set_title('Nodes Colored by Local Clustering')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('facebook_network_sample.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Network sample visualization saved as 'facebook_network_sample.png'\")\n",
    "    \n",
    "    def compare_with_random_network(self):\n",
    "        \"\"\"Compare Facebook network with equivalent random networks.\"\"\"\n",
    "        if self.facebook_graph is None:\n",
    "            return\n",
    "        \n",
    "        G = self.facebook_graph\n",
    "        n = G.number_of_nodes()\n",
    "        m = G.number_of_edges()\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPARISON WITH RANDOM NETWORKS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Generate equivalent random networks\n",
    "        print(\"Generating comparison networks...\")\n",
    "        \n",
    "        # Erdős-Rényi with same density\n",
    "        p = 2 * m / (n * (n - 1))\n",
    "        er_graph = nx.erdos_renyi_graph(n, p, seed=42)\n",
    "        \n",
    "        # Configuration model (same degree sequence)\n",
    "        degree_sequence = [d for n, d in G.degree()]\n",
    "        # Ensure sum is even for configuration model\n",
    "        if sum(degree_sequence) % 2 == 1:\n",
    "            degree_sequence[0] += 1\n",
    "        \n",
    "        try:\n",
    "            config_graph = nx.configuration_model(degree_sequence, seed=42)\n",
    "            config_graph = nx.Graph(config_graph)  # Remove multi-edges and self-loops\n",
    "            config_graph.remove_edges_from(nx.selfloop_edges(config_graph))\n",
    "        except:\n",
    "            config_graph = er_graph  # Fallback\n",
    "        \n",
    "        # Calculate properties\n",
    "        networks = {\n",
    "            'Facebook (Real)': G,\n",
    "            'Erdős-Rényi': er_graph,\n",
    "            'Configuration Model': config_graph\n",
    "        }\n",
    "        \n",
    "        properties = {}\n",
    "        for name, graph in networks.items():\n",
    "            props = {}\n",
    "            props['nodes'] = graph.number_of_nodes()\n",
    "            props['edges'] = graph.number_of_edges()\n",
    "            props['avg_degree'] = 2 * graph.number_of_edges() / graph.number_of_nodes()\n",
    "            props['clustering'] = nx.average_clustering(graph)\n",
    "            props['transitivity'] = nx.transitivity(graph)\n",
    "            \n",
    "            # Path length on largest component\n",
    "            if nx.is_connected(graph):\n",
    "                props['avg_path_length'] = nx.average_shortest_path_length(graph)\n",
    "            else:\n",
    "                largest_cc = max(nx.connected_components(graph), key=len)\n",
    "                subgraph = graph.subgraph(largest_cc)\n",
    "                if subgraph.number_of_nodes() > 1:\n",
    "                    props['avg_path_length'] = nx.average_shortest_path_length(subgraph)\n",
    "                else:\n",
    "                    props['avg_path_length'] = 0\n",
    "            \n",
    "            properties[name] = props\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(f\"\\n{'Network':<20} {'Nodes':<8} {'Edges':<8} {'Avg Degree':<12} {'Clustering':<12} {'Path Length':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for name, props in properties.items():\n",
    "            print(f\"{name:<20} {props['nodes']:<8} {props['edges']:<8} \"\n",
    "                  f\"{props['avg_degree']:<12.2f} {props['clustering']:<12.4f} \"\n",
    "                  f\"{props['avg_path_length']:<12.2f}\")\n",
    "        \n",
    "        # Calculate ratios\n",
    "        fb_clustering = properties['Facebook (Real)']['clustering']\n",
    "        er_clustering = properties['Erdős-Rényi']['clustering']\n",
    "        \n",
    "        print(f\"\\nKey Comparisons:\")\n",
    "        print(f\"  • Facebook clustering is {fb_clustering/er_clustering:.1f}x higher than random\")\n",
    "        print(f\"  • This demonstrates the 'small-world' property of social networks\")\n",
    "        \n",
    "        return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e2911",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run Facebook network exploration.\"\"\"\n",
    "    print(\"Facebook Network Explorer\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Initialize explorer\n",
    "    explorer = FacebookNetworkExplorer()\n",
    "    \n",
    "    # Load Facebook network\n",
    "    if not explorer.load_facebook_network():\n",
    "        print(\"Failed to load Facebook network. Please check the data file.\")\n",
    "        return\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    explorer.analyze_basic_properties()\n",
    "    explorer.analyze_degree_distribution()\n",
    "    explorer.calculate_centralities()\n",
    "    explorer.visualize_network_sample(sample_size=150)\n",
    "    explorer.compare_with_random_network()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Facebook Network Analysis Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Generated files:\")\n",
    "    print(\"  • facebook_degree_analysis.png\")\n",
    "    print(\"  • facebook_centrality_analysis.png\")\n",
    "    print(\"  • facebook_network_sample.png\")\n",
    "    print(\"\\nThis analysis demonstrates key properties of real-world social networks:\")\n",
    "    print(\"  • High clustering (friends of friends are often friends)\")\n",
    "    print(\"  • Small-world properties (short paths despite high clustering)\")\n",
    "    print(\"  • Scale-free degree distribution (few highly connected hubs)\")\n",
    "    print(\"  • Community structure and centrality patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07329771",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
